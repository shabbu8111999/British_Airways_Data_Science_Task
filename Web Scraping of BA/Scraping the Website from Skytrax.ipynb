{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c80873",
   "metadata": {},
   "source": [
    "# Scraping the Website from Skytrax "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f629948",
   "metadata": {},
   "source": [
    "First of all I imported Necessary libraries for Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4f30dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c705b7",
   "metadata": {},
   "source": [
    "I used this URL as mentioned in the work stats and just got them their respective size so that the process should go well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b3e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "BA_url = 'https://www.airlinequality.com/airline-reviews/british-airways'\n",
    "pages = 40\n",
    "page_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef63589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty list on reviews, stars, date, country \n",
    "\n",
    "reviews = []\n",
    "stars = []\n",
    "date = []\n",
    "country = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8b62ec",
   "metadata": {},
   "source": [
    "This is the Mandatory process to Extract the Text Content from the Web Page, I used BeautifulSoup to Scrap the HTML Content as that was mentioned in the Inspect Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a223b61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Page 1\n",
      "Scraping Page 2\n",
      "Scraping Page 3\n",
      "Scraping Page 4\n",
      "Scraping Page 5\n",
      "Scraping Page 6\n",
      "Scraping Page 7\n",
      "Scraping Page 8\n",
      "Scraping Page 9\n",
      "Scraping Page 10\n",
      "Scraping Page 11\n",
      "Scraping Page 12\n",
      "Scraping Page 13\n",
      "Scraping Page 14\n",
      "Scraping Page 15\n",
      "Scraping Page 16\n",
      "Scraping Page 17\n",
      "Scraping Page 18\n",
      "Scraping Page 19\n",
      "Scraping Page 20\n",
      "Scraping Page 21\n",
      "Scraping Page 22\n",
      "Scraping Page 23\n",
      "Scraping Page 24\n",
      "Scraping Page 25\n",
      "Scraping Page 26\n",
      "Scraping Page 27\n",
      "Scraping Page 28\n",
      "Scraping Page 29\n",
      "Scraping Page 30\n",
      "Scraping Page 31\n",
      "Scraping Page 32\n",
      "Scraping Page 33\n",
      "Scraping Page 34\n",
      "Scraping Page 35\n",
      "Scraping Page 36\n",
      "Scraping Page 37\n",
      "Scraping Page 38\n",
      "Scraping Page 39\n",
      "Scraping Page 40\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, pages + 1):\n",
    "    print(f'Scraping Page {i}')\n",
    "    \n",
    "    # Creating a URL to collect the links and resize them for process\n",
    "    url = f\"{BA_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "    \n",
    "    # Collecting HTML data from this page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "     # Extracting reviews\n",
    "    for para in parsed_content.find_all('div',{'class':'text_content'}):\n",
    "        reviews.append(para.get_text())\n",
    "        \n",
    "     # Extracting stars\n",
    "    for star_div in parsed_content.find_all('div', {'class': 'rating-10'}):\n",
    "        try:\n",
    "            stars.append(star_div.span.text)\n",
    "        except AttributeError:\n",
    "            print(f\"Error on page {i} - No star rating found\")\n",
    "            stars.append(\"None\")\n",
    "            \n",
    "    # Extracting date\n",
    "    for time_tag in parsed_content.find_all('time'):\n",
    "        date.append(time_tag.text)\n",
    "    \n",
    "    # Extracting country\n",
    "    for h3_tag in parsed_content.find_all('h3'):\n",
    "        try:\n",
    "            country.append(h3_tag.span.next_sibling.text.strip(\" ()\"))\n",
    "        except AttributeError:\n",
    "            print(f\"Error on page {i} - No country found\")\n",
    "            country.append(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56517551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Total Reviews\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(reviews)} Total Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dcbf5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040 Total Stars\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(stars)} Total Stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c3d538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Total Date\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(date)} Total Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f90c6358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 Total Country\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(country)} Total Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a18f5",
   "metadata": {},
   "source": [
    "# Used Truncated Method because the size was not perfect for the Stars Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "26f48d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(len(reviews), len(stars), len(date), len(country))\n",
    "\n",
    "# Truncating the lists to the minimum length\n",
    "reviews = reviews[:min_length]\n",
    "stars = stars[:min_length]\n",
    "date = date[:min_length]\n",
    "country = country[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e285fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97a4fe27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21249085",
   "metadata": {},
   "source": [
    "# Imported them into the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "348c094b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Stars(Ratings)</th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  Booked a flight from Buchar...</td>\n",
       "      <td>\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t5</td>\n",
       "      <td>12th November 2023</td>\n",
       "      <td>Romania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  Booked online months ago an...</td>\n",
       "      <td>1</td>\n",
       "      <td>8th November 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  The flight was on time. The...</td>\n",
       "      <td>8</td>\n",
       "      <td>7th November 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified |  Angry, disappointed, and unsat...</td>\n",
       "      <td>7</td>\n",
       "      <td>5th November 2023</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  As an infrequent flyer, Bri...</td>\n",
       "      <td>2</td>\n",
       "      <td>5th November 2023</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  \\\n",
       "0  ✅ Trip Verified |  Booked a flight from Buchar...   \n",
       "1  ✅ Trip Verified |  Booked online months ago an...   \n",
       "2  ✅ Trip Verified |  The flight was on time. The...   \n",
       "3  Not Verified |  Angry, disappointed, and unsat...   \n",
       "4  ✅ Trip Verified |  As an infrequent flyer, Bri...   \n",
       "\n",
       "                  Stars(Ratings)                Date         Country  \n",
       "0  \\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t5  12th November 2023         Romania  \n",
       "1                              1   8th November 2023  United Kingdom  \n",
       "2                              8   7th November 2023  United Kingdom  \n",
       "3                              7   5th November 2023           Italy  \n",
       "4                              2   5th November 2023  United Kingdom  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Reviews' : reviews,\n",
    "    'Stars(Ratings)' : stars,\n",
    "    'Date' : date,\n",
    "    'Country' : country\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9b746",
   "metadata": {},
   "source": [
    "# Converting into CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c14afb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('BA_Feedbacks.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
